### ResNet 网络结构

---
一般认为神经网络的每一层分别对应于提取不同层次的特征信息，有低层，中层和高层，而网络越深的时候，提取到的不同层次的信息会越多，而不同层次间的层次信息的组合也会越多。

深度学习对于网络深度遇到的主要问题是[梯度消失和梯度爆炸](https://blog.csdn.net/qq_25737169/article/details/78847691)，传统对应的解决方案则是数据的初始化(normlized initializatiton)和（batch normlization）正则化，但是这样虽然解决了梯度的问题，深度加深了，却带来了另外的问题，就是网络性能的退化问题，深度加深了，错误率却上升了，而残差用来设计解决退化问题，其同时也解决了梯度问题，更使得网络的性能也提升了。~~Hinton提出的胶囊网络未来可能会抛弃反向传播彻底解决此问题~~

![](img/8-1.jpg)

**深度残差学习**

深度残差学习，其中分为三个小部分，包括 残差元，为什么是恒等映射？ 如何快捷映射不是恒等的情况？

![](img/8-2.png)<center>图3  残差网络的构思</center>

对于普通网络，任意堆叠的两层网络，我们希望找到的是一个映射H(x)对应的残差元，我们添加一个快捷连接，从输入到输出，这里的快捷连接默认为恒等映射，此时的问题就从寻找映射H（x）到F(x)；这里类似于在数学上，你直接去解一个方程较为复杂，你就把它分解成两个简单问题和的形式，分别去解决。

![](img/8-3.png)<center>图 4 残差网络的设计统计学来源</center>

因此，对应于残差的初始定义，在统计学中，为实际观测值与估计值（拟合值）的差值，这里则是直接的映射H(x)与快捷连接x的差值。

**残差网络**：对于残差元的主要设计有两个，快捷连接和恒等映射，快捷连接使得残差变得可能，而恒等映射使得网络变深，而恒等映射主要有两个：快捷连接为恒等映射 和 相加后的激活函数

![](img/8-4.png)<center>图 5 残差网络的设计构思</center>

这样设计的主要思想：去构造映射H(x)，与构造残差映射F(x）是等价的，此外残差映射也更容易优化。

![](img/8-5.png)  

这里考虑若这个激活函数也为恒等映射时，则变为下式：
![](img/8-6.png)<center>图 6 基本残差单元的介绍</center>

若after-add后的激励函数也为恒等映射时，灰化的线，表示恒等映射了；

![](img/8-7.png)<center>图 7残差网络的前向传播</center>

 前向过程，最后的结果表示直接的前向过程，连加的运算（考虑的残差元为一个单元，残差元的内部还是两层的连乘），即从第l层可以直接到第L层，而传统的网络则是连乘运算，计算量明显不同。（从连乘到连加）

 
