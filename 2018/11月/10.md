### Aggregated Residual Transformations for Deep Neural Networks

### ResNeXt网络

***
**摘要**

本文提出了一个简单的，高度模块化的网络结构，用于图像分类---ResNeXt网络。
网络通过重复一个block来构建，这个block聚合了一组有相同拓扑的转换。我们的简单设计产生了一个同构的、多分支的 架构，它只需要设置几个超参数。这个策略揭示了一个新的维度，我们称之为"基数-cardinality"(这组转换的大小)，除了深度和宽度之外的一个关键因子。在ImageNet-1K数据集上，我们的实验表明，在控制复制度的受限情况下，增加基数可以提升分类精度。而且当提升模型容量时(即增加模型复杂度)，提升基数比更深或更宽更加有效。

---
**ResNet与ResNeXt比较**

ResNet特点：
- ResNet的核心就是shortcut的出现。图1是ResNet的基本模块。
- ResNet的贡献就是引入了旁路（shortcut）。旁路（shortcut）的引入一方面使得梯度的后向传播更加容易，使更深的网络得以有效训练；另一方面，也使得每个基本模块的学习任务从学习一个绝对量变为学习相对上一个基本模块的偏移量。

![ResNet](img/ResNet.png)

上图如果去掉旁路，那就是经典的堆叠式的网络。

ResNet网络结构：

![network](img/Resnetwork.png)


<br><br><br><br>
ResNeXt特点：
- 在严格保证计算复杂度时，增加Cardinality(基数)能够提高图像分类精度，并且，增加Cardinality比加深网络或加宽网络更加有效
- 与ResNet相比，相同的精度，ResNeXt计算量更小。参数更少，ResNeXt-50 接近ResNet-101的准确度（当然ResNeXt-101的精度更高）
- ResNeXt网络模块化设计更加合理，结构更简单，超参数量更少

网络结构：<br><br>
![ResNext](img/ResNeXt.jpg)

左图是ResNet的基本模块，右图是ResNeXt的基本模块。容易发现，ResNeXt就是把ResNet的单个卷积改成了多支路的卷积。

作者说ResNeXt的目的是为了保持ResNet的高可移植性优点，同时继续提高精度。文章中还顺便吐槽了Inception的缺点。

***
**补充知识**

VGG-Nets/ResNets: 堆叠相同形状的网络 构造blocks    -----相当于增加深度depth

Incptions：split-transform-merge（分割-转换-融合（聚合）），将输入采用（1*1 Conv）分裂为几个低维embedding，在经过一系列特定filters（如3X3,5X5）的变换，最后连接在一起

ResNeXt：采用Vggs/ResNets的网络depth 加深方式，同时利用splot-transform-merge策略

基数(Cardinality,一组转换的数量)是除了宽度和高度之外具体的、可度量的维度，并且是核心重要的。实验证明增加基数是比变得更深或更宽更加有效的提升精度的方法，特别是当深度和宽度开始使现有模型的收益递减时。


[论文地址](https://arxiv.org/pdf/1611.05431.pdf)
