### YOLO



       本文提出了一个新的物体检测的方法：You Only Look
Once（YOLO）。之前的物体检测方法通常都转变为了一个分类问题，如 R-CNN、Fast R-CNN
等等。YOLO将检测看作一个 regression problem，YOLO 从输入的图像，仅仅经过一个
neural network，直接预测 bounding boxes 以及每个 bounding box
所属类别的概率。正因为整个的检测过程仅仅有一个网络，所以它可以直接 end-to-end
的优化。

       YOLO 结构十分的快，标准的 YOLO 版本每秒可以实时地处理 45
帧图像。一个较小版本：Fast YOLO，可以每秒处理 155 帧图像，它的 mAP（mean Average
Precision） 依然可以达到其他实时检测算法的两倍。同时相比较于其他的 state-of-art
detection systems。尽管 YOLO 的定位更容易出错，这里的 定位出错，即是指
coordinate errors。但是 YOLO 在背景预测上有更少的
false-positive(即是背景而预测为某种对象的错误情况较少)。

        最后，YOLO可以学习到物体的更加泛化的特征，在将 YOLO
用到其他领域的图像时（如 artwork 的图像上），其检测效果要优于 DPM、R-CNN
这类方法。

论文地址：<https://arxiv.org/abs/1506.02640>

参考：[论文阅读笔记：You Only Look Once: Unified, Real-Time Object
Detection](http://blog.csdn.net/tangwei2014/article/details/50915317)


**创新点**

创新点：  端到端训练及推断 + 改革区域建议框式目标检测框架 + 实施目标检测

针对简单图片检测速度快，准确率高

**缺点**

- 因为每个 grid cell 中只能预测两个 boxes，以及有一个类别。限制了YOLO对相邻物体的检测能力
- 同一类物体出现新的、不常见的长宽比时，YOLO 的泛化能力较弱。
- 对图片中过大过小的图片采取同等对待，造成误差较大。YOLO 中最主要的误差仍是定位不准造成的误差。


![YOLO](img/YOLO_1.png)

YOLO的核心思想就是利用整张图作为网络的输入，直接在输出层回归bounding
box的位置和bounding box所属的类别。

 a) 将一幅图像分成SxS个网格(grid cell)，如果某个object的中心
落在这个网格中，则这个网格就负责预测这个object。 

 b) 每个网格要预测B个bounding box，每个bounding
box除了要回归自身的位置之外，还要附带预测一个Confidence值。

c) Confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息；计算公式Confidence=$P_r(Object) * IOU^{truth}_{pred}​$; 所以在gird
cell中没有对象,那么Confidence为0；如果有对象就是与GT的IOU值。(注：Confidence代表bounding
box与任意的GT box的IOU)。

d) 每个bounding box要预测(x, y, w, h)和confidence共5个值。

e) 每个网格还要预测一个class类别信息$P_r(Class_i|Object)$；这是一个条件概率，在包含对象情况下，属于某类对象的概率。(注：class信息是针对每个网格的，confidence信息是针对每个bounding
box的）

        对于SxS个网格，每个网格要预测B个bounding
box还要预测C个categories。输出就是S x S x (5\*B+C)的一个tensor。


![YOLO_2](img/YOLO_2.png)

**YOLO的核心公式--损失函数**

![公式](img/YOLO_3.png)


    a) YOLO使用 sum-squared error 来作为优化目标，因为sum-squared error
较容易优化，但是对于本问题，直接使用 sum-squared error
却不是那么合适。主要有两个问题

-   localization error和classification error的权重一样，不是理想结果。

-   每一张图像中，很多 grid cells 并没不包含物体，使得这些 cells 的 confidence
    置为 0，这些不包含物体的 grid cells
    的梯度更新，将会以压倒性的优势，覆盖掉包含物体的 grid cells
    进行的梯度更新。这些问题会使得模型不稳定，甚至造成网络的发散。

     为了避免上述的问题，将 localization error 以及 classification error 的 loss
重新用权重衡量。以平衡上述的失衡问题。简单的说，就是增加 bounding box coordinate
的 loss，减少不包含物体 grid cells 的 confidence 的 loss；增加两个参数λcoord = 5
和λnoobj = 0.5。

      b) Sum-squared error loss 中还将 large box 与 small box 的 loss
同等对待。因为 box 与 groundtruth 的小的偏离，
对于较大的box，结果的影响较小；对小的box影响较大。为了解决这个问题，用了一个很巧妙的
trick，即最后并不是直接输出 bounding box 的 width、height，而是 width、height
的square root，平方根。

 

       c) 每个 grid cell 预测多个 bounding
box，但在网络模型的训练中，每一个物体，只想要一个 bounding box predictor
来负责预测。因此， 当前哪一个 predictor 预测的 prediction 与 groundtruth 的 IoU
最大 ，这个 predictor 就负责 predict object。因此只有在当 bounding box predictor
对 groundtruth box 负责的时候，才会对 bounding box coordinate error 进行惩罚。

        d) loss function 在只有当一个 grid cell 中存在 object 时，才会对
classification error 进行惩罚。



### Mask-RCNN

Mask R-CNN是在 Faster R-CNN 上的扩展——在其已有的用于边界框识别分支上添加了一个并行的用于预测目标掩码的分支。Mask R-CNN的训练很简单，只是在R-CNN的基础增加了少量的计算量，预测速度大约为5fps。另外，Mask R-CNN能够更好地适用于其他任务，例如：估计同一图片中人物的姿态，本文在COCO挑战中的3种任务（包括实例分割、边界框目标探测、任务关键点检测）种都获得了最好的成绩。在没有适用其他技巧的情况下，Mask R­-CNN的表现超过了现有的单个模型，包括COCO 2016挑战赛的获胜模型。本报道将会简要从提出背景、关键概念、Mask R­-CNN简介以及相关实验结果四个发面进行介绍。

Mask R-CNN就是Faster R-CNN 加上一个用于像素语义分割的FCN。

Msak RCNN整体架构

![MASKRCNN](img/MaskRCNN.png)

![MskRCNN2](img/MskRCNN2.png)

Faster RCNN整体架构

![Faster-RCNN](img/Faster-RCNN.png)

**相关知识**

1、Instance Segmentation-实例分割

![instance-seg](img/instance-seg.png)

instance segmentation（也称为Simultaneous Detection and Segmentation）.  它要解决的问题是检测（Object Detection）和语义分割（Semantic Segmentation）综合的一个问题。比起检测，需要得到物体更精确的边界信息；比起语义分割，需要区分不同的物体个体。上图图清楚说明了image classification, object detection, semantic segmentation, instance segmentation之间的关系。

2、Feature Pyramid Networks-特征金字塔网络

作者利用了固有的多尺度和锥形层次结构的卷积网络来构造具有边际额外成本的金字塔网络，开发具有侧向连接（Skip Connector）的自顶向下的架构，用于在所有尺度上构建高级语义特征图，依靠一种通过自上而下的路径和横向连接将低分辨率但语义强的特征与高分辨率语义弱的特征结合在一起，这样就可以获得高分辨率，强语义的特征，有利于小目标的检测。这种结构是在CNN网络中完成的，和前文提到的基于图片的金字塔结构不同，而且完全可以替代它。

![](img/FPN.png)
![](img/FPN2.png)

自上而下的路径和横向连接

自上而下的路径是如何去结合低层高分辨率的特征呢？方法就是，把更抽象，语义更强的高层特征图进行取样，然后把该特征横向连接（lateral connections）至前一层特征，因此高层特征得到加强。值得注意的是，横向连接的两层特征在空间尺寸上要相同，这样做主要是为了利用低层的定位细节信息。

把高层特征做2倍上采样（最近邻上采样法），然后将其和对应的前一层特征结合（前一层要经过1*1的卷积核才能用，目的是改变channels，应该是要和后一层的channels相同），结合的方式就是像素间的加法。重复迭代该过程，直至生成最精细的特征图。迭代开始阶段，作者在C5层后面加了一个1*1的卷积核来产生最粗略的特征图，最后，作者用3*3的卷积核去处理已经融合的特征图（为了消除上采样的混叠效应），以生成最后需要的特征图。{C2，C3，C4，C5}层对应融合特征层为{P2，P3，P4，P5}，对应的层空间尺寸是相通的。
![](img/FPN3.png)

3、ResNeXt网络

详见：[ResNeXt网络笔记](https://github.com/BMDACMER/paper/blob/master/2018/11%E6%9C%88/10.md)

4、ROI Align

ROI Pooling这一操作存在两次量化的过程。

将候选框边界量化为整数点坐标值。

将量化后的边界区域平均分割成 k x k 个单元(bin),对每一个单元的边界进行量化。

经过上述两次量化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度。

      如 下图 所示，这是一个Faster-RCNN检测框架。输入一张800*800的图片，图片上有一个665*665的包围框(框着一只狗)。图片经过主干网络提取特征后，特征图缩放步长（stride）为32。因此，图像和包围框的边长都是输入时的1/32。800正好可以被32整除变为25。但665除以32以后得到20.78，带有小数，于是ROI Pooling 直接将它量化成20。接下来需要把框内的特征池化7*7的大小，因此将上述包围框平均分割成7*7个矩形区域。显然，每个矩形区域的边长为2.86，又含有小数。于是ROI Pooling 再次把它量化到2。经过这两次量化，候选区域已经出现了较明显的偏差（如图中绿色部分所示）。更重要的是，该层特征图上0.1个像素的偏差，缩放到原图就是3.2个像素。那么0.8的偏差，在原图上就是接近30个像素点的差别，这一差别不容小觑。
![](img/ROI_1.png)

ROI Align的思路很简单：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作。值得注意的是，在具体的算法操作上，ROI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，如 下图 所示：

遍历每一个候选区域，保持浮点数边界不做量化。

将候选区域分割成k x k个单元，每个单元的边界也不做量化。

在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。

![](img/ROI_2.png)


### 图像难度预测器

Optimizing the Trade-off between Single-Stage and Two-Stage Object Detectors using Image Difficulty Prediction

结合YoLo与Mask RCNN建立灵活的包含两种模型的目标检测模型----图像难度预测器

主要有两种类型的最先进的物体探测器。
- 一方面，我们two-stage探测器，比如Faster R-CNN（基于区域的卷积神经网络）或Mask R-CNN，它们（i）使用区域提议网络在第一阶段产生兴趣区域， （ii）将区域提案发送到pipeline中以进行对象分类和边界框回归。这样的模型达到了最高的准确率，但通常较慢。
- 另一方面，我们有YOLO（You Only Look Once）和SSD（Singe Shot MultiBox Detector）等single-stage探测器，将物体探测作为一个简单的回归问题，它将输入图像作为输入图像并学习类概率，边界框坐标。这种模型的准确率较低，但比two-stage物体探测器快得多。

 使用图像难度预测器来实现目标检测中精度和速度之间的最佳平衡。将图像难度预测器应用到测试图像上，将其分解为简易图像和困难图像。一旦分离后，简单的图像将被发送到更快的single-stage探测器，而困难图像被发送到更精确的two-stage探测器。

 在PASCAL VOC 2007上进行的实验表明，使用图像难度与图像的随机分割相比毫不逊色。我们的方法是灵活的，因为它允许选择一个期望的阈值，将图像分成简单和较难。


**参考文献**

[YOLO](https://arxiv.org/abs/1506.02640)<br>
YOLO参考博客：https://blog.csdn.net/tangwei2014/article/details/50915317 <br>
[MAsk RCNN](https://arxiv.org/pdf/1703.06870.pdf)<br>
